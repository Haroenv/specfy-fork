---
id: 1
slug: efficient-dockerfile-nodejs
title: Building efficient NodeJS Docker image
description: A step by step guide to build fast and lightweight NodeJS docker images.
publishedAt: "2023-07-10"
---

Docker is an invaluable tool for managing containers, but it can be **challenging to grasp** its inner workings, especially for newcomers. While modern hosting platforms like Vercel and Netlify often eliminate the need for writing Dockerfiles for NodeJS processes, understanding Docker optimization becomes crucial when handling your infrastructure. By achieving optimal efficiency, you can **save both time and money** in the long run.


## A Brief Overview of Docker's Internals

Before diving into the optimization techniques, let's briefly explore how Docker operates internally. If you're primarily interested in the end result, feel free to skip this section.
Docker employs a **layered approach**, where each modification to the filesystem creates a new layer on top of the previous one. Imagine a delicious mille-feuille pastry, where each layer adds to the overall flavor. Except **you actually want less** flavor in this scenario.

```dockerfile
# Creates a Layer
FROM node

# Creates a Layer
WORKDIR /app/tmp

# Creates a Layer
RUN apk update && apk add curl

# Creates a Layer
COPY . /app/tmp

RUN echo "hello world"
```

This layering strategy allows Docker to **cache** the results of each command. However, there is a caveat: these layers are included in the final Docker image during the build process. Consequently, if unnecessary layers are present in the build, and the image undergoes multiple intermediate steps, more layers will be **transmitted over the wire**.


## Context

In a typical NodeJS build process within Docker, several steps are involved:

1. Use the correct Node base image
2. Copy the source code
3. Install the dependencies
4. Build/Compile the source code
5. Run the process

While these steps are generally necessary, it's worth examining if all of them are truly required.
Upon closer inspection, we can identify two layers in the process that can be eliminated from the final Docker image. First, the step of "copying the source code" becomes irrelevant once we have compiled it, as the source is no longer utilized. Second, the installation of "devDependencies" is only necessary during development and is not required to run the process in a production environment.

To illustrate these optimization techniques, let's consider an example using Specfy's own backend, which utilizes common tools such as Eslint, Typescript, Fastify, Vite, and Prisma. I will be using a Macbook Pro 2023 M2 14".

Now, let's delve into the optimization steps.


## 1. Initial Dockerfile

Now that we understand the context and the optimization goals, we can begin by creating a simple Dockerfile that launches our NodeJS process.

Here's our base:

```dockerfile
FROM node:18.16.1

WORKDIR /app/tmp

# Copy source code
COPY . /app/tmp

# Install the dependencies
RUN npm install

# Build the source code
RUN true \
  && cd pkgs/api \
  && npm run prod:build

EXPOSE 8080
```

<br />

<Banner type="info">To build from scratch every time you need to `docker system prune` and `docker build --pull --no-cache ./`</Banner>

The output will look like this:

```bash
[+] Building 106.8s (10/10) FINISHED
=> [internal] load build definition from Dockerfile                                                                   0.0s
=> => transferring dockerfile: 200B                                                                                   0.0s
=> [internal] load .dockerignore                                                                                      0.0s
=> => transferring context: 35B                                                                                       0.0s
=> [internal] load metadata for docker.io/library/node:18.16.1                                                        37.8s
=> [1/5] FROM docker.io/library/node:18.16.1@sha256:f4698d49371c8a9fa7dd78b97fb2a532213903066e47966542b3b1d403449da4  8.8s
=> => resolve docker.io/library/node:18.16.1@sha256:f4698d49371c8a9fa7dd78b97fb2a532213903066e47966542b3b1d403449da4  0.0s
=> => sha256:52cd8b9f214029f30f0e95d330c83ab4499ebafaac853fdce81a8511a1bdafd7 45.60MB / 45.60MB                       7.8s
=> => sha256:f0eeafef34cdd8f38ffee08ec08b9864563684021057b8e61120c237dd108442 2.28MB / 2.28MB                         1.4s
=> => sha256:55a1a1affff207f32bb4b7c3efcff310d55215d1302bb3308099a079177154e7 450B / 450B                             1.4s
=> => sha256:f4698d49371c8a9fa7dd78b97fb2a532213903066e47966542b3b1d403449da4 1.21kB / 1.21kB                         0.0s
=> => sha256:9ffaff1b88cf5635b1b79123c9c0bbf79813da17ec6b28a7b215cab4d797a384 2.00kB / 2.00kB                         0.0s
=> => sha256:8e80a3bf661ba38d6d8b92729057dddd71531831f774f710cb4bf66c818a370c 7.26kB / 7.26kB                         0.0s
=> => extracting sha256:52cd8b9f214029f30f0e95d330c83ab4499ebafaac853fdce81a8511a1bdafd7                              0.8s
=> => extracting sha256:f0eeafef34cdd8f38ffee08ec08b9864563684021057b8e61120c237dd108442                              0.1s
=> => extracting sha256:55a1a1affff207f32bb4b7c3efcff310d55215d1302bb3308099a079177154e7                              0.0s
=> [internal] load build context                                                                                      0.1s
=> => transferring context: 68.01kB                                                                                   0.1s
=> [2/5] WORKDIR /app/tmp                                                                                             0.2s
=> [3/5] COPY . /app/tmp                                                                                              0.1s
=> [4/5] RUN npm install                                                                                             47.4s
=> [5/5] RUN true   && cd pkgs/api   && npm run prod:build                                                            8.6s
=> exporting to image                                                                                                10.3s
=> => exporting layers                                                                                               10.3s
=> => writing image sha256:b025ac6558b78d16781531ba1590abcdeb8c1fe98243cef383938b3f2b40c007                           0.0s
=> => naming to docker.io/library/test                                                                                0.0s
```


**Results**:
<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "200px"}}></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td  className="px-4">106.8s</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td  className="px-4">69s</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source", "Cached"],
        datasets: [
          {
            label: 'Original Time to build',
            data: [106.8, 69, 0.9],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
        ],
      }}
    />
  </div>
</div>

It's worth noting that Docker cache hit is highly efficient and can significantly reduce the build time, often eliminating around 99% of the initial build time.

However, one drawback is the large size of the final image, which **currently stands at 2.48GB**. This size is far from optimal and should be addressed to improve efficiency.

In the next steps, we will focus on optimizing the Docker build process to reduce the image size while maintaining the necessary functionality.

## 2. Using Slim or Alpine images

First thing we can do is to use Slim or Alpine image.
While [Alpine](https://www.docker.com/blog/how-to-use-the-alpine-docker-official-image) images are often recommended for their lightweight nature, it's important to exercise caution, especially with NodeJS applications.

Alpine images utilize musl libc, which operates slightly differently from the regular glibc. This difference can introduce issues such as segfaults, memory problems, DNS issues, and more, particularly when working with NodeJS. For example, at my previous job Algolia, we faced DNS errors while using Alpine in the Crawler (as mentioned in my [blog post](https://www.algolia.com/blog/engineering/30-days-to-improve-our-crawler-performance-by-50-percent/)) and Segfault when using Isolated V8.

Considering these concerns, it's advisable to choose **Slim** images, which offer **similar benefits** in terms of reduced image size without compromising compatibility or introducing potential issues.

```dockerfile
FROM node:18.16.1-bullseye-slim

WORKDIR /app/tmp

# Copy source code
COPY . /app/tmp

# Install the dependencies
RUN npm install

# Build the backend
RUN true \
  && cd pkgs/api \
  && npm run prod:build

EXPOSE 8080
```

**Results**:
<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "150px"}}></th><th></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td className="px-4">106.8s</td><td className="px-4">57.5s (-47%)</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td className="px-4">69s</td><td className="px-4">45s (-34%)</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td><td className="px-4">1.0s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td><td className="px-4">1.61GB (-46%)</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source"],
        datasets: [
          {
            label: 'Original',
            data: [106.8, 69],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
          {
            label: 'Slim image',
            data: [57.5, 45],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
        ],
      }}
    />
  </div>
</div>

That was an easy step and we managed to reduce our image size by 46%, almost a GB, I don't know exactly what was removed but my mind is shaking.
Additionally, the build time improved by 47% (~40seconds), especially considering that it was achieved with just one line of modification.
Please mind that this base image is still ~230MB (uncompressed), so can never go reach lower than that.

## 3. Multi Stage build

We now turn our attention to the multi-stage builds tricks. For those unfamiliar with this concept, multi-stage builds are like **portals within a Dockerfile**.
You can simply change the context entirely but you are allowed to keep some things with you, and only the final stage will be sent.
This is perfect to copy the compiled source and completely removes `node_modules` or remove some part of the source code.

<Banner type="info">Running `RUN rm -rf node_modules` will actually create a new layer of 0B and not reduce the final image size</Banner>

```dockerfile
# Base image
FROM node:18.16.1-bullseye-slim as tmp

WORKDIR /app/tmp

# Copy source code
COPY . /app/tmp

# Install the dependencies
RUN npm install

# Build the backend
RUN true \
  && cd pkgs/api \
  && npm run prod:build

# Final image
FROM node:18.16.1-bullseye-slim as web

# BONUS: Do not use root to run the app
USER node

WORKDIR /app

COPY --from=tmp --chown=node:node /app/tmp /app

EXPOSE 8080
```

On this step we have changed a bit more things, we added a second `FROM` that tells Docker this is a multi-staged build and a `COPY`.
This is portal we were talking about, we created a second image, where the only filesystem operation is a copy. So we removed all others intermediate layers and if what we copy is small we saved a lot.


**Results**:
<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "150px"}}></th><th></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td className="px-4">106.8s</td><td className="px-4">65.2s (-38%)</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td className="px-4">69s</td><td className="px-4">50s (-27%)</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td><td className="px-4">1.2s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td><td className="px-4">1.32GB (-46%)</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source"],
        datasets: [
          {
            label: 'Original',
            data: [106.8, 69],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
          {
            label: 'Slim image',
            data: [57.5, 45],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Multi-Staged',
            data: [65.2, 50],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
        ],
      }}
    />
  </div>
</div>

In this example I copied everything from the filesystem, but we still managed to **save ~300MB**, at the relative cost of few seconds of build time.


## 4. Caching Dependencies

You will have notice at this point, installing the dependencies is the main **bottleneck**. This step often takes around 30 seconds, even when the dependencies themselves haven't changed.
That's because if your repo change it will invalidate all the subsequent cached layers as soon as you do `COPY . /app/tmp`
To address this issue, we can introduce another multi-stage image to leverage caching effectively. Let's examine the updated Dockerfile:

```dockerfile
# package.json cache
FROM alpine:latest as deps

RUN apk add --no-cache bash curl wget jq

# Each package.json needs to copied manually unfortunately
COPY package.json /tmp
COPY pkgs/api/package.json /tmp/api_package.json
COPY pkgs/app/package.json /tmp/app_package.json
COPY pkgs/website/package.json /tmp/website_package.json

# Base image
FROM node:18.16.1-bullseye-slim as tmp

WORKDIR /app/tmp

RUN mkdir -p ./pkgs/api/
RUN mkdir -p ./pkgs/api/
RUN mkdir -p ./pkgs/website/

# Copy and install dependencies separately from the app's code
# To leverage Docker's cache when no dependency has change
COPY --from=deps /tmp/package.json ./package.json
COPY --from=deps /tmp/api_package.json ./pkgs/api/package.json
COPY --from=deps /tmp/app_package.json ./pkgs/app/package.json
COPY --from=deps /tmp/website_package.json ./pkgs/website/package.json

COPY package-lock.json  ./

# Install every dependencies
RUN true \
  && npm install

# Copy source code
COPY . /app/tmp

# Build the backend
RUN true \
  && cd pkgs/api \
  && npm run prod:build

# Final image
FROM node:18.16.1-bullseye-slim as web

# BONUS: Do not use root to run the app
USER node

WORKDIR /app

COPY --from=tmp --chown=node:node /app/tmp /app

EXPOSE 8080
```

In this enhanced Dockerfile, we introduce a new stage named `deps`. This stage solely copies the `package.json` files, enabling us to leverage Docker's caching mechanism effectively.

The result is a **significant improvement** in build time, particularly when only the source code has been modified. With this approach, we achieved an incredible 30 seconds reduction in build time, 50% improvement from the previous iteration and 71% improvement overall. It's important to note that these steps are now cached, except for the `npm run build` step, which can never be cached except if there is absolutely no change.


**Results**:
<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "150px"}}></th><th></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td className="px-4">106.8s</td><td className="px-4">62.0s (-41%)</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td className="px-4">69s</td><td className="px-4">20s (-71%)</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td><td className="px-4">1.5s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td><td className="px-4">1.32GB (-46%)</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source"],
        datasets: [
          {
            label: 'Original',
            data: [106.8, 69],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
          {
            label: 'Slim image',
            data: [57.5, 45],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Multi-Staged',
            data: [65.2, 50],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Cached Dependencies',
            data: [62.0, 20],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
        ],
      }}
    />
  </div>
</div>

> Cached output example

```bash
[...]
=> CACHED [tmp  3/13] RUN mkdir -p ./pkgs/api/                                                                        0.0s
=> CACHED [tmp  4/13] RUN mkdir -p ./pkgs/api/                                                                        0.0s
=> CACHED [tmp  5/13] RUN mkdir -p ./pkgs/website/                                                                    0.0s
=> CACHED [deps 2/6] RUN apk add --no-cache bash curl wget jq                                                         0.0s
=> CACHED [deps 3/6] COPY package.json /tmp                                                                           0.0s
=> CACHED [deps 4/6] COPY pkgs/api/package.json /tmp/api_package.json                                                 0.0s
=> CACHED [deps 5/6] COPY pkgs/app/package.json /tmp/app_package.json                                                 0.0s
=> CACHED [deps 6/6] COPY pkgs/website/package.json /tmp/website_package.json                                         0.0s
=> CACHED [tmp  6/13] COPY --from=deps /tmp/package.json ./package.json                                               0.0s
=> CACHED [tmp  7/13] COPY --from=deps /tmp/api_package.json ./pkgs/api/package.json                                  0.0s
=> CACHED [tmp  8/13] COPY --from=deps /tmp/app_package.json ./pkgs/app/package.json                                  0.0s
=> CACHED [tmp  9/13] COPY --from=deps /tmp/website_package.json ./pkgs/website/package.json                          0.0s
=> CACHED [tmp 10/13] COPY package-lock.json  ./                                                                      0.0s
=> CACHED [tmp 11/13] RUN true   && npm install                                                                       0.0s
=> [tmp 12/13] COPY . /app/tmp                                                                                        0.1s
=> [tmp 13/13] RUN true   && cd pkgs/api   && npm run prod:build                                                      8.5s
=> CACHED [web 2/3] WORKDIR /app                                                                                      0.0s
=> [web 3/3] COPY --from=tmp --chown=node:node /app/tmp /app                                                          9.4s
=> exporting to image                                                                                                 5.8s
[...]
```


## 5. Cleaning dependencies

While we have made significant progress in optimizing the build time, achieving a sub-20-second build time may prove challenging due to the compilation itself that often takes 10-15 seconds.
At this stage, we must address the issue of the image size, which currently stands at 1.32GB. This size is far from ideal and can impact deployment efficiency, resource consumption, and scalability.


### 5.1 Removing devDependencies

We can remove the development dependencies that are no longer required once the source code has been built. This step ensures that we only retain the relevant dependencies needed for running the application, while discarding tools like Typescript, Webpack, and other development-specific dependencies.

```dockerfile
# Clean dev dependencies
RUN true \
  && npm prune --omit=dev
```

With a simple one-liner, we have achieved remarkable results in terms of image size reduction. Approximately 53% or around 600MB of dependencies have been eliminated from the final image. One might say that the NodeJS ecosystem is bloated, but I still love it very much.

**Results**:
<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "150px"}}></th><th></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td className="px-4">106.8s</td><td className="px-4">60.6 (-43%)</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td className="px-4">69s</td><td className="px-4">18s (-73%)</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td><td className="px-4">0.8s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td><td className="px-4">613MB (-75%)</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source"],
        datasets: [
          {
            label: 'Original',
            data: [106.8, 69],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
          {
            label: 'Slim image',
            data: [57.5, 45],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Multi-Staged',
            data: [65.2, 50],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Cached Dependencies',
            data: [62.0, 20],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
          {
            label: 'Removed devDependencies',
            data: [60.6, 18],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
        ],
      }}
    />
  </div>
</div>


### 5.2 Pre-Filtering

An even better approach is to avoid downloading unnecessary dependencies altogether. This is the most challenging aspect of our optimization process as it goes beyond the scope of Docker itself.

To accomplish this, I have chosen to implement a bash script that removes unnecessary dependencies before the `npm install` command is executed. Alternatively, you can also consider using a different package.json file specifically tailored for production use. Unfortunately, unlike in Ruby, there is currently no built-in way to tag dependencies and selectively install them. Therefore, we must rely on manual intervention to achieve our goal.

Dependencies that can be safely removed are: formatting libraries, testing libraries, release libraries, and any other libraries that are only required during development and not at compile time.

To simplify this process, I have created a small bash script that utilizes JQ to transform the package.json file. In addition to removing `devDependencies`, I have also removed other fields that are typically irrelevant and can be modified without affecting the installation.


```bash
#!/usr/bin/env bash

# clean_package.json.sh

jq 'walk(if type == "object" then with_entries(select(.key | test("^jest|prettier|stylelint|turbowatch|eslint|semantic|dotenv|nodemon|renovate") | not)) else . end) | { name, type, dependencies, devDependencies, packageManager, workspaces }' <"$1" >"${1}_tmp"
mv "${1}_tmp" "$1"
```

```dockerfile
# package.json cache
FROM alpine:latest as deps

RUN apk add --no-cache bash curl wget jq

# To prevent cache invalidation from changes in fields other than dependencies
COPY prod/clean_package_json.sh /tmp

# Each package.json needs to copied manually unfortunately
COPY package.json /tmp
RUN /tmp/clean_package_json.sh /tmp/package.json
COPY pkgs/api/package.json /tmp/api_package.json
RUN /tmp/clean_package_json.sh /tmp/api_package.json
COPY pkgs/app/package.json /tmp/app_package.json
RUN /tmp/clean_package_json.sh /tmp/app_package.json
COPY pkgs/website/package.json /tmp/website_package.json
RUN /tmp/clean_package_json.sh /tmp/website_package.json

[...]
```

I have now a very small stage to manipulate the `package.json` that can be executed in parallel. While these layers will always be invalidated due to the modification of the file system, their impact is minimal as they are short-lived.

We don't save on the final image size since all `devDependencies` were already ignored, but we saved a few seconds (about 5 seconds or 10% decrease) on the initial install time and we save on intermediate layers size. This step is definitely far less impacting but always good to take.

<Banner type="info"><strong>Good to know:</strong> if you don't use this technique and you increment the `version` field when you release, it will invalidate the cache every time.</Banner>

## 6. Own your stack

The last step is much more personal because there is no low hanging fruit anymore, yet my image still weights 613MB (uncompressed).
To further refine the image, we need to delve into the layers themselves and identify any significant dependencies that may have inadvertently made their way into the image.

Breakdown:
- My code: 11MB
- Docker Image + Layer: 240MB
- Dependencies: 362MB

Yes, more than 300Mb of "production" dependencies. The biggest offenders being:

```bash
712K	  node_modules/bluebird
744K	  node_modules/zod
900K	  node_modules/bottleneck
952K	  node_modules/pino
1.0M	  node_modules/markdown-it
1.1M	  node_modules/js-beautify
1.4M	  node_modules/@babel
1.5M	  node_modules/acorn-node
1.6M	  node_modules/ts-node
1.9M	  node_modules/@manypkg
2.7M	  node_modules/ajv-formats
2.7M	  node_modules/env-schema
3.2M	  node_modules/@swc
3.2M	  node_modules/fast-json-stringify
4.2M	  node_modules/fastify
4.4M	  node_modules/@types
4.4M	  node_modules/react-dom
4.5M	  node_modules/sodium-native
5.0M	  node_modules/lodash
6.6M	  node_modules/tw-to-css
6.7M	  node_modules/@fastify
8.7M	  node_modules/react-email
12M   node_modules/@specfy
14M   node_modules/prisma
17M   pkgs/api/node_modules/.prisma
19M   node_modules/@octokit
39M   node_modules/@prisma
65M   node_modules/typescript
```

In there, unfortunately a lot of uncompressable bloated dependencies.
One notable example is the `@octokit` package, which occupies a significant amount of space at 19MB, despite being used for only four (4) REST API calls.

`Prisma` is surprinsingly big and duplicated for some reason which amounts to a **stagering 50Mb**, to do some Database call.

Furthermore some packages are there because others packages are wrongly installing their dependencies as prod dependencies unfortunately.
For example, Next.js always installs Typescript as prod dependency. After realising this and also wanting to have a CDN, I moved all my Frontends to Vercel.
Removing `app` and `website` from my repo saved another 50MB.

Removing peer dependencies, while it can be risky depending on the number and importance of the packages that rely on them, can be another effective strategy for reducing the image size. In my case, removing peer dependencies resulted in a size reduction of 47MB.

```dockerfile
# Clean dev dependencies
RUN true \
  && npm prune --omit=dev --omit=peer
```

## Final results

After implementing various optimization techniques, we achieved notable improvements in both build time and image size. Here are the key findings:

<div className="flex gap-8 mb-8">
  <table className="border rounded-md text-sm h-full">
    <thead>
      <tr><th style={{ width: "150px"}}></th><th></th><th></th></tr>
    </thead>
    <tbody>
      <tr><td className="py-2 px-4">From scratch</td><td className="px-4">106.8s</td><td className="px-4">60.6 (-43%)</td></tr>
      <tr><td className="py-2 px-4">Code Modified</td><td className="px-4">69s</td><td className="px-4">18s (-73%)</td></tr>
      <tr><td className="py-2 px-4">Cache hit</td><td className="px-4">0.9s</td><td className="px-4">0.8s</td></tr>
      <tr><td className="py-2 px-4">Image Size </td><td className="px-4">2.48GB</td><td className="px-4">513MB (-79%)</td></tr>
    </tbody>
  </table>


  <div className="grow">
    <ChartBar
      data={{
        labels: ["From scratch", "Modified Source"],
        datasets: [
          {
            label: 'Original',
            data: [106.8, 69],
            backgroundColor: 'hsl(206 81.9% 65.3%)',
          },
          {
            label: 'Final',
            data: [60.6, 18],
            backgroundColor: 'hsl(131 38.1% 56.3%)',
          },
        ],
      }}
    />
  </div>
</div>

**Build Time Savings**:
- Fresh build: saved 40 seconds, representing a 43% decrease in build time.
- Regular build: significant time reduction of 51 seconds, which corresponds to an impressive **73% decrease in build time**.

**Image Size Reduction**:
  - The compiled image size was reduced by 1.9GB, resulting in an **80% decrease in size**.

Although these results are significant, there is still a sense of bittersweetness. It becomes apparent that many of the optimization steps undertaken should ideally be automated. Additionally, despite the achieved improvements, the final image size remains relatively large for an API.

To remember:
- Modifications to the repository that are not ignored by `.dockerignore` will break the cache, necessitating a complete rebuild.
- **Deleting files** during the build will **not save size**
- Utilizing a **multi-stage build** approach simplifies the process of eliminating unnecessary layers.
- **Cleaning up dependencies** plays a crucial role




## Bonus

Just a few points that were not covered:

- Removing the source code is usually not that impactful. By adding this, I saved around 2MB.

```dockerfile
# Clean src
RUN true \
  && rm -rf pkgs/*/src
```

- If you compile a frontend in your dockerfile be sure to clean cache file, usually it's saved for reuse in the `node_modules` folder but can be deleted.

- [depot.dev](http://depot.dev), while being very promising is a paid / remote solution.


